# Scrapper Putusan-MA-RI

## ğŸ”§ Pembaruan Terakhir (2025-08-07)

### âœ… Perbaikan Scraper 
- **Kompatibilitas urllib3 2.0+**: Fixed TypeError untuk `method_whitelist` â†’ `allowed_methods`
- **Error Handling Comprehensive**: Menangani timeout, rate limiting, connection errors
- **Fallback Strategy**: Requests â†’ Selenium untuk anti-bot protection
- **Debug & Monitoring**: Logging detail, progress tracking, checkpoint system
- **User Agent Rotation**: Menghindari detection dengan rotasi user agent
- **HTML Structure Analysis**: Debug tools untuk analisis struktur website

### ğŸš€ Quick Start

```bash
# Test konektivitas
python scraper.py --test-only --debug

# Scrape 10 halaman dengan format JSON
python scraper.py --pages 10 --format json

# Resume dari checkpoint
python scraper --resume --pages 50

# Debug mode dengan HTML analysis
python run_analysis.py --debug
```

### ğŸ“– Dokumentasi Lengkap
- [ğŸ“‹ Panduan Lengkap Scraper](SCRAPER_GUIDE.md)
- [ğŸ’» Contoh Penggunaan](example_usage.py)
- [ğŸ” Debug Tools](test_debug.py)

## ğŸ› ï¸ Requirements

Pastikan menggunakan versi library terbaru:

```bash
pip install -r requirements.txt
```

Library utama:
- `requests >= 2.28.0`
- `urllib3 >= 2.5.0` (compatible)
- `beautifulsoup4 >= 4.11.0`
- `selenium >= 4.8.0`
- `pandas >= 1.5.0`
- `tqdm >= 4.64.0`

## ğŸ“ Struktur Proyek

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py              # Main scraper (UPDATED)
â”‚   â”œâ”€â”€ config.py               # Konfigurasi sistem
â”‚   â”œâ”€â”€ utils.py                # Utility functions
â”‚   â”œâ”€â”€ debug_tools.py          # Debug dan monitoring tools (NEW)
â”‚   â””â”€â”€ html_structure_analyzer.py # HTML analysis tools (NEW)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Data mentah hasil scraping
â”‚   â””â”€â”€ processed/              # Data yang sudah diproses
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ scraper_*.log          # Log files scraping
â”‚   â””â”€â”€ html_debug/            # Debug HTML files (NEW)
â”‚       â”œâ”€â”€ requests_index_*.html
â”‚       â””â”€â”€ requests__direktori_*.html
â”œâ”€â”€ example_usage.py           # Contoh penggunaan (NEW)
â”œâ”€â”€ run_analysis.py            # Script analisis (NEW)
â”œâ”€â”€ test_debug.py              # Debug testing (NEW)
â”œâ”€â”€ SCRAPER_GUIDE.md           # Panduan lengkap (NEW)
â””â”€â”€ requirements.txt           # Dependencies
```

## ğŸ” Error Handling & Debugging

### Error Scenarios yang Ditangani
- âœ… **DNS resolution failures**: Automatic retry dengan backoff
- âœ… **Connection timeouts**: Configurable timeout settings
- âœ… **HTTP error codes**: 403, 404, 429, 500+ dengan retry logic
- âœ… **Rate limiting**: Exponential backoff strategy
- âœ… **SSL/TLS errors**: Certificate validation handling
- âœ… **Anti-bot protection**: Selenium WebDriver fallback

### Debug Features
- ğŸ” **HTML Debug Saving**: Menyimpan response HTML untuk analisis
- ğŸ“Š **Progress Tracking**: Real-time monitoring dengan tqdm
- ğŸ“ˆ **Statistics Collection**: Success/error rates dan performance metrics
- ğŸ”„ **Checkpoint System**: Resume scraping dari titik terakhir
- ğŸ“ **Comprehensive Logging**: Multi-level logging (DEBUG, INFO, WARNING, ERROR)

## ğŸ“Š Features Utama

### Modern Scraping Infrastructure
- **urllib3 2.0+ Compatible**: Menggunakan `allowed_methods` pattern terbaru
- **Automatic Retry Logic**: Exponential backoff dengan jitter
- **User Agent Rotation**: Mencegah detection dengan rotasi UA
- **Session Management**: Persistent cookies dan headers

### Data Processing & Analysis
- **HTML Structure Analysis**: Tools untuk memahami struktur website
- **Multiple Output Formats**: JSON, CSV, Excel support
- **Data Validation**: Schema validation untuk data integrity
- **Incremental Processing**: Efficient data processing dengan chunking

### Monitoring & Observability
- **Real-time Progress**: Live progress bars dan statistics
- **Performance Metrics**: Request timing dan success rates
- **Error Analytics**: Detailed error categorization dan reporting
- **Debug Artifacts**: HTML saves dan request/response logging

## ğŸ¯ Target Website Analysis

Scraper ini dirancang khusus untuk website Mahkamah Agung Indonesia:

### Endpoint yang Didukung
- `https://putusan3.mahkamahagung.go.id/` - Homepage
- `https://putusan3.mahkamahagung.go.id/direktori.html` - Direktori putusan
- `https://putusan3.mahkamahagung.go.id/direktori/index/kategori/*` - Kategori specific
- `https://putusan3.mahkamahagung.go.id/pengadilan/index/ditjen/*` - Per peradilan

### Struktur Data yang Diextract
- **Metadata Putusan**: Nomor, tanggal, pengadilan
- **Kategori Hukum**: Perdata, pidana, TUN, agama, dll
- **Statistik**: View count, download count, status
- **Hierarki Pengadilan**: MA, tinggi, negeri, agama, militer, pajak

## ğŸš€ Quick Start Examples

### 1. Test Konektivitas
```bash
python scraper.py --test-only --debug --verbose
```

### 2. Scraping Basic
```bash
# Scrape 50 halaman dengan retry logic
python scraper.py --pages 50 --format json --retry-attempts 3
```

### 3. Advanced Scraping
```bash
# Full scraping dengan semua features
python scraper.py \
    --pages 100 \
    --format excel \
    --debug \
    --save-html \
    --checkpoint-interval 10 \
    --delay-range 1,3
```

### 4. Resume dari Checkpoint
```bash
python scraper.py --resume --pages 200
```

### 5. Debug Mode
```bash
# Analisis struktur HTML
python run_analysis.py --analyze-structure --save-debug

# Test dengan Selenium fallback
python test_debug.py --use-selenium
```

## ğŸ“ˆ Performance & Monitoring

### Statistics Tracking
- **Success Rate**: Percentage keberhasilan request
- **Average Response Time**: Waktu rata-rata response
- **Error Distribution**: Breakdown error berdasarkan type
- **Throughput**: Pages per minute processed

### Checkpoint System
- Auto-save progress setiap N pages
- Resume capability dari titik terakhir
- Backup data integrity checks
- Rolling checkpoint dengan retention

### Memory Management
- Efficient data structures untuk large datasets
- Garbage collection optimization
- Memory usage monitoring
- Configurable batch processing

## ğŸ”§ Configuration

### Environment Variables
```bash
export SCRAPER_USER_AGENT="Custom User Agent"
export SCRAPER_DELAY_MIN=1
export SCRAPER_DELAY_MAX=3
export SCRAPER_TIMEOUT=30
export SCRAPER_RETRY_ATTEMPTS=3
```

### Config File (src/config.py)
```python
# Request settings
DEFAULT_TIMEOUT = 30
MAX_RETRY_ATTEMPTS = 3
DELAY_RANGE = (1, 3)

# Output settings
DEFAULT_OUTPUT_FORMAT = 'json'
CHECKPOINT_INTERVAL = 50

# Debug settings
SAVE_DEBUG_HTML = True
DEBUG_LOG_LEVEL = 'INFO'
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

#### 1. urllib3 Compatibility Error
```
TypeError: 'method_whitelist' parameter is not supported.
```
**Solution**: Update ke urllib3 >= 2.0 dan gunakan `allowed_methods`

#### 2. Connection Timeout
```
requests.exceptions.ConnectTimeout
```
**Solution**: Increase timeout, check network, enable debug mode

#### 3. Rate Limiting (429)
```
HTTP 429 Too Many Requests
```
**Solution**: Increase delay range, enable exponential backoff

#### 4. Anti-bot Detection
```
HTTP 403 Forbidden
```
**Solution**: Enable Selenium fallback, rotate user agents

### Debug Commands
```bash
# Full debug dengan HTML save
python example_usage.py --debug --save-html --verbose

# Network diagnostics
python test_debug.py --test-network --timeout 60

# HTML structure analysis
python run_analysis.py --analyze-structure
```

## ğŸ“Š Output Formats

### JSON Output
```json
{
  "metadata": {
    "scraped_at": "2025-08-07T15:30:00Z",
    "pages_processed": 50,
    "success_rate": 96.0
  },
  "data": [
    {
      "nomor_putusan": "123/Pdt.G/2025/PN.Jkt",
      "pengadilan": "Pengadilan Negeri Jakarta",
      "kategori": "Perdata",
      "tanggal": "2025-01-15",
      "status": "Berkekuatan Hukum Tetap"
    }
  ]
}
```

### CSV Output
```csv
nomor_putusan,pengadilan,kategori,tanggal,status
123/Pdt.G/2025/PN.Jkt,Pengadilan Negeri Jakarta,Perdata,2025-01-15,Berkekuatan Hukum Tetap
```

## ğŸ”’ Best Practices

### Rate Limiting
- Gunakan delay 1-3 detik antar request
- Monitor response headers untuk rate limit info
- Implement exponential backoff untuk errors

### Data Integrity
- Validate data structure sebelum save
- Implement checksum untuk file integrity
- Backup checkpoint files secara berkala

### Error Handling
- Log semua errors dengan context
- Implement graceful degradation
- Monitor error rates dan patterns

## ğŸ“š References

- [Mahkamah Agung Website](https://putusan3.mahkamahagung.go.id/)
- [Requests Documentation](https://docs.python-requests.org/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium Documentation](https://selenium-python.readthedocs.io/)

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Implement changes dengan tests
4. Update documentation
5. Submit pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Support

Untuk pertanyaan atau issues:
- Create GitHub issue dengan detail error
- Include log files dan debug output
- Specify environment (OS, Python version, dependencies)

---

**Last Updated**: 2025-08-07  
**Version**: 2.0.0  
**Compatibility**: Python 3.8+, urllib3 2.0+
