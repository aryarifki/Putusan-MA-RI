#!/usr/bin/env python3
"""
Example script untuk menggunakan scraper Mahkamah Agung
"""

import sys
import os
import argparse
from datetime import datetime

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from scraper import MahkamahAgungScraper

def main():
    parser = argparse.ArgumentParser(description='Scraper Mahkamah Agung RI')
    parser.add_argument('--pages', type=int, default=5, help='Number of pages to scrape')
    parser.add_argument('--start-page', type=int, default=1, help='Starting page number')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    parser.add_argument('--format', choices=['json', 'csv', 'excel'], default='json', help='Output format')
    parser.add_argument('--output', type=str, help='Output filename (without extension)')
    parser.add_argument('--test-only', action='store_true', help='Test connectivity only')
    parser.add_argument('--resume', action='store_true', help='Resume from checkpoint')
    
    args = parser.parse_args()
    
    print("ğŸ›ï¸  Scraper Mahkamah Agung RI")
    print("=" * 50)
    print(f"Debug mode: {'ON' if args.debug else 'OFF'}")
    print(f"Output format: {args.format}")
    
    # Initialize scraper
    scraper = MahkamahAgungScraper(debug=args.debug)
    
    try:
        if args.test_only:
            print("\nğŸ” Testing connectivity...")
            test_url = "https://putusan3.mahkamahagung.go.id"
            content = scraper.get_page_content(test_url)
            
            if content:
                print(f"âœ… Successfully connected to {test_url}")
                print(f"ğŸ“„ Content length: {len(content):,} characters")
                
                # Test parsing
                sample_data = scraper.parse_putusan_list(content)
                print(f"ğŸ“Š Sample data extracted: {len(sample_data)} items")
                
                if sample_data and args.debug:
                    print("\nğŸ“‹ Sample data preview:")
                    for i, item in enumerate(sample_data[:3]):
                        print(f"  {i+1}. {item}")
            else:
                print(f"âŒ Failed to connect to {test_url}")
                return
        
        else:
            print(f"\nğŸš€ Starting scraping...")
            print(f"ğŸ“– Pages: {args.start_page} to {args.start_page + args.pages - 1}")
            
            # Start scraping
            data = scraper.scrape_pages(
                start_page=args.start_page,
                max_pages=args.start_page + args.pages - 1,
                resume_from_checkpoint=args.resume
            )
            
            print(f"\nâœ… Scraping completed!")
            print(f"ğŸ“Š Total records: {len(data)}")
            
            # Show statistics
            stats = scraper.get_stats()
            print(f"ğŸ“ˆ Success rate: {stats['success_rate']:.1f}%")
            print(f"ğŸ”¢ Total requests: {stats['total_requests']}")
            print(f"âœ… Successful: {stats['successful_requests']}")
            print(f"âŒ Failed: {stats['failed_requests']}")
            
            if stats['method_used']:
                print(f"ğŸ› ï¸  Methods used: {stats['method_used']}")
            
            # Save data
            if data:
                output_name = args.output or f"putusan_ma_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                scraper.save_data(output_name, format=args.format)
                print(f"ğŸ’¾ Data saved as {output_name}.{args.format}")
            else:
                print("âš ï¸  No data to save")
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Scraping interrupted by user")
        stats = scraper.get_stats()
        if stats['total_requests'] > 0:
            print(f"ğŸ“Š Partial results: {len(scraper.scraped_data)} records")
            scraper.save_data("interrupted_run", format=args.format)
            print("ğŸ’¾ Partial data saved")
    
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
    
    finally:
        scraper.cleanup()
        print("\nğŸ§¹ Cleanup completed")

if __name__ == "__main__":
    main()